# EDA Prompt Engine


本リポジトリは、`pandas.DataFrame.describe()` 形式の統計量を入力として、
構造化された EDA（探索的データ分析）レポートを生成するための
**バージョン管理付き Prompt エンジン**です。

本プロジェクトの特徴は、Prompt を単なる文章ではなく、
**設計資産（プロダクトアーティファクト）として扱うこと**にあります。

> Prompt を「設計仕様」として扱うための実験的エンジニアリングプロジェクト。

---

## 🎓 想定読者

- LLM を業務に組み込もうとしているデータサイエンティスト
- Prompt を再現可能な仕様として扱いたいエンジニア
- LLM の出力品質・回帰検出に関心がある人

--- 

## 📊 プロジェクトの状態

- 実験フェーズ：v4 を安定版として運用中
- 単体テスト：Titanic describe ケース
- 手動回帰チェック：実施済

---

## 🎯 コンセプト

LLM を利用した分析では、出力の品質や再現性が問題になります。

本リポジトリでは：

- Prompt を仕様として明示化する
- バージョン管理する
- テストケースを用意する
- 回帰チェックを行う
- モデル差分を比較する

というソフトウェアエンジニアリングの考え方を、
Prompt 設計に適用しています。

---

## リポジトリ構成

- `prompts/`
  - バージョン管理された Prompt 仕様
- `tests/`
  - テスト入力データ（describe形式）
- `evaluations/`
  - モデル出力・比較・回帰チェック
- `prompt_spec.md`
  - 入出力仕様書
- `CHANGELOG.md`
  - 変更履歴

---

## 🛠 対象とする問題

入力：
- describe() 相当の統計量テキスト

出力：
- 型分類
- 欠損分析
- 外れ値候補
- 推測（事実と分離）
- 最小実験セット（評価指標付き）

---

## 📌 本プロジェクトの目的

- Prompt の進化を履歴として残す
- 設計バグを可視化する
- LLM 出力の回帰を検出する
- ガバナンス耐性のある分析出力を設計する
- Prompt を DSL（設計仕様）として扱う実践例を示す

---

## 🚀 使用方法（手動実行）

1. `prompts/eda_latest.md` を開く
2. 内容を LLM（例：ChatGPT）に貼り付け
3. `tests/titanic_summary.txt` を入力
4. 出力を `evaluations/` に保存
5. `regression_checklist.md` で確認

---

## 🏷 現在の安定版

- 最新安定版：`eda_v4`
- 基本テストケース：Titanic describe summary
- 基準モデル：ChatGPT 5.2（UIデフォルト設定）

---

## 🔍 なぜこのプロジェクトが重要か

多くの Prompt 集は「良い書き方の例」を提示するだけです。

本プロジェクトは：

- Prompt を設計対象として扱う
- ルールベース化する
- 型分岐・IQR基準などを明示する
- 成功判定まで含める
- 回帰チェックを行う

という点で、**Prompt Engineering をソフトウェア工学に接続する試み**です。

---

## 🔮 今後の展開

- 追加テストケース（Housing, Wine 等）
- モデル間差分比較
- API 実行による自動回帰テスト
- Prompt DSL の形式仕様化
- CI 連携

---

## 🧠 作者メモ

このリポジトリは、

「LLMをどう使うか」ではなく  
「LLMをどう設計するか」を実験するための場です。